# ADR-0004: Deep Research Gap Analysis â€” Estado Actual vs Arquitectura CDR

**Fecha:** 2025-01-XX  
**Estado:** Draft  
**Autor:** Claude (anÃ¡lisis tÃ©cnico)  
**Contexto:** AuditorÃ­a completa del mÃ³dulo Deep Research en MedeX para identificar brechas con la arquitectura CDR objetivo.

---

## 1. Executive Summary

La implementaciÃ³n actual de Deep Research en MedeX es funcional pero **fundamentalmente diferente** de la arquitectura CDR (Clinical Deep Research) propuesta. El sistema actual es un **buscador con sÃ­ntesis LLM**, mientras que CDR es un **sistema de investigaciÃ³n estructurado con trazabilidad total**.

### Veredicto

| Aspecto                 | Actual                                       | CDR Target                                 | Gap        |
| ----------------------- | -------------------------------------------- | ------------------------------------------ | ---------- |
| Arquitectura            | MonolÃ­tica, un paso                          | 11 capas orquestadas                       | ğŸ”´ CrÃ­tico |
| Trazabilidad            | Ninguna (afirmaciones sin fuente especÃ­fica) | Total (cada claim con snippet verificable) | ğŸ”´ CrÃ­tico |
| ExtracciÃ³n estructurada | No existe                                    | StudyCards con PICO                        | ğŸ”´ CrÃ­tico |
| EvaluaciÃ³n de sesgo     | No existe                                    | RoB2 formal                                | ğŸ”´ CrÃ­tico |
| VerificaciÃ³n            | No existe                                    | Capa de verificaciÃ³n obligatoria           | ğŸ”´ CrÃ­tico |
| OrquestaciÃ³n            | Secuencial simple                            | Grafo con estados y gates                  | ğŸ”´ CrÃ­tico |
| Fuentes                 | PubMed + Semantic Scholar                    | PubMed + ClinicalTrials.gov + Full-text    | ğŸŸ¡ Parcial |
| Retrieval               | Keyword simple                               | Hybrid (BM25 + Dense + Rerank)             | ğŸ”´ CrÃ­tico |
| Output                  | Texto libre del LLM                          | Report estructurado con claims verificados | ğŸ”´ CrÃ­tico |

**ConclusiÃ³n:** Se requiere una **reimplementaciÃ³n completa**, preservando solo algunos componentes bÃ¡sicos de retrieval como punto de partida.

---

## 2. AnÃ¡lisis Detallado por Componente

### 2.1 Capa de OrquestaciÃ³n

#### Estado Actual

```
scientific_search.py â†’ perform_scientific_research()
â”‚
â”œâ”€â”€ PubMedClient.search() â†’ PMIDs
â”œâ”€â”€ PubMedClient.fetch_articles() â†’ XML parsing
â”œâ”€â”€ SemanticScholarClient.search()
â”œâ”€â”€ [Optional] DuckDuckGo fallback
â”œâ”€â”€ Deduplicate + Sort by evidence
â””â”€â”€ Build prompt â†’ Call LLM â†’ Return text

state.py â†’ start_research()
â”‚
â”œâ”€â”€ Llama a perform_scientific_research()
â”œâ”€â”€ Construye prompt con build_scientific_research_prompt()
â”œâ”€â”€ POST a /api/v1/query
â””â”€â”€ Muestra resultado como texto
```

**Problemas:**

- Flujo secuencial sin checkpoints
- Sin estado persistente entre pasos
- Sin capacidad de retry/rollback
- Sin gates de validaciÃ³n
- Sin paralelizaciÃ³n controlada

#### Arquitectura CDR Requerida (LangGraph)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   INTERFACE     â”‚
                    â”‚  (Question In)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”Œâ”€â”€â”€â”€â”€â”€â–ºâ”‚  ORCHESTRATION  â”‚â—„â”€â”€â”€â”€â”€â”€â”
            â”‚       â”‚   (LangGraph)   â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚  PICO GATE âœ“    â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
       RETRYâ”‚                â”‚                â”‚VERIFY
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚FAIL
            â”‚       â”‚    RETRIEVAL    â”‚       â”‚
            â”‚       â”‚ BM25+Dense+Rerankâ”‚      â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚   SCREENING     â”‚       â”‚
            â”‚       â”‚  (In/Exclude)   â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚    PARSING      â”‚       â”‚
            â”‚       â”‚ (Snippet Extract)â”‚      â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚   STUDYCARDS    â”‚       â”‚
            â”‚       â”‚ (PICO Extract)  â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚     ROB2        â”‚       â”‚
            â”‚       â”‚ (Bias Assess)   â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚    SKEPTIC      â”‚       â”‚
            â”‚       â”‚ (Challenge)     â”‚       â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â”‚       â”‚   SYNTHESIS     â”‚       â”‚
            â”‚       â”‚ (Generate Claims)â”‚      â”‚
            â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
            â”‚                â”‚                â”‚
            â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”‚  VERIFICATION   â”‚â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  (Fact Check)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PUBLISHER     â”‚
                    â”‚ (Report + Traces)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Gap:** ğŸ”´ CRÃTICO â€” No existe orquestaciÃ³n. Flujo lineal sin estados.

---

### 2.2 Capa de Retrieval

#### Estado Actual

**PubMedClient (scientific_search.py:109-275)**

```python
class PubMedClient:
    BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"

    async def search(query, max_results=10) -> list[str]:  # PMIDs
        # Simple keyword search
        params = {"db": "pubmed", "term": query, "retmax": max_results}

    async def fetch_articles(pmids) -> list[ScientificArticle]:
        # XML parsing of abstracts only
```

**Problemas:**

- Solo keyword search, no query expansion
- No BM25 scoring
- No embeddings/vector search
- No reranking
- No full-text retrieval (solo abstracts)
- No ClinicalTrials.gov
- No MeSH term expansion

#### Arquitectura CDR Requerida

```python
class HybridRetriever:
    """Retrieval pipeline: BM25 + Dense + Reranker"""

    def __init__(self):
        self.bm25 = BM25Index()
        self.dense = DenseRetriever(model="PubMedBERT")
        self.reranker = CrossEncoder("ms-marco-MiniLM")

    async def retrieve(self, query: str, pico: PICOQuery) -> list[Record]:
        # 1. Expand query with MeSH terms
        expanded = await self.mesh_expand(query)

        # 2. BM25 retrieval (title + abstract)
        bm25_results = self.bm25.search(expanded, k=100)

        # 3. Dense retrieval (semantic similarity)
        dense_results = self.dense.search(query, k=100)

        # 4. Fusion (RRF or weighted)
        fused = self.reciprocal_rank_fusion(bm25_results, dense_results)

        # 5. Rerank top candidates
        reranked = self.reranker.rerank(query, fused[:50])

        return reranked[:20]  # Top 20 with scores
```

**Gap:** ğŸ”´ CRÃTICO â€” Solo keyword search, sin hybrid retrieval.

---

### 2.3 Modelo de Datos

#### Estado Actual (scientific_search.py)

```python
@dataclass
class ScientificArticle:
    pmid: str = ""
    doi: str = ""
    title: str = ""
    authors: list = field(default_factory=list)
    journal: str = ""
    year: int = 0
    abstract: str = ""
    mesh_terms: list = field(default_factory=list)
    article_type: str = ""
    url: str = ""
    evidence_level: EvidenceLevel = EvidenceLevel.UNKNOWN
    citation_count: int = 0
    source_api: str = ""
```

**Problemas:**

- No hay estructura PICO
- No hay snippets con posiciÃ³n
- No hay scoring de retrieval
- No hay hash para deduplicaciÃ³n confiable
- No hay NCT IDs para clinical trials
- No hay distinction entre record y study card

#### Arquitectura CDR Requerida (Pydantic)

```python
from pydantic import BaseModel, Field
from typing import Optional
from enum import Enum

class Record(BaseModel):
    """Raw retrieved record before processing"""
    record_id: str = Field(..., description="Unique ID: PMID, NCT, or DOI")
    title: str
    authors: list[str]
    year: int
    doi: Optional[str] = None
    pmid: Optional[str] = None
    nct_id: Optional[str] = None
    abstract: str
    full_text: Optional[str] = None
    url: str
    source: Literal["pubmed", "clinicaltrials", "semantic_scholar"]
    hash: str = Field(..., description="SHA256 of title+abstract for dedup")
    retrieval_scores: dict[str, float] = Field(
        default_factory=dict,
        description="bm25_score, dense_score, rerank_score"
    )

class Snippet(BaseModel):
    """Extracted text fragment with location"""
    text: str
    record_id: str
    location: str  # "abstract:sentence_3" or "fulltext:section_methods:para_2"
    char_start: int
    char_end: int

class StudyCard(BaseModel):
    """Structured extraction from a study"""
    record_id: str
    study_type: StudyType  # RCT, Cohort, CaseControl, etc.

    # PICO extracted
    population: str
    intervention: str
    comparator: Optional[str]
    outcomes: list[str]

    # Results
    sample_size: Optional[int]
    follow_up: Optional[str]
    primary_endpoint: Optional[str]
    effect_size: Optional[str]
    confidence_interval: Optional[str]
    p_value: Optional[str]

    # Supporting evidence
    supporting_snippets: list[Snippet]

    # Quality
    evidence_level: EvidenceLevel
    rob2_result: Optional[RoB2Result]

class EvidenceClaim(BaseModel):
    """A claim with mandatory source attribution"""
    claim_text: str
    source_refs: list[str]  # List of record_ids
    supporting_snippets: list[Snippet]
    confidence: float  # 0-1
    uncertainty_note: Optional[str]

class CDRState(BaseModel):
    """Complete state of a CDR session"""
    session_id: str
    question: str
    pico: Optional[PICOQuery]
    search_plan: Optional[SearchPlan]
    retrieved_records: list[Record] = []
    screened: ScreeningResult = None
    study_cards: list[StudyCard] = []
    rob2_results: list[RoB2Result] = []
    claims: list[EvidenceClaim] = []
    answer: Optional[str] = None
    report: Optional[str] = None
    traces: list[TraceEntry] = []

    # Metadata
    created_at: datetime
    updated_at: datetime
    status: SessionStatus
```

**Gap:** ğŸ”´ CRÃTICO â€” Modelo de datos plano sin estructura PICO, sin snippets, sin claims verificables.

---

### 2.4 ExtracciÃ³n Estructurada (StudyCards)

#### Estado Actual

**NO EXISTE.** Los artÃ­culos se pasan como texto plano al LLM que genera un resumen libre.

```python
# scientific_search.py lÃ­nea ~720
def _synthesize_articles(articles, web_results):
    """Synthesize articles into content for LLM analysis."""
    parts = []
    for i, article in enumerate(articles[:15], 1):
        parts.append(
            f"[Ref {i}] {evidence_tag}\n"
            f"TÃ­tulo: {article.title}\n"
            f"Abstract: {article.abstract[:500]}...\n"  # TRUNCADO!
        )
    return "\n---\n".join(parts)
```

**Problemas:**

- Abstracts truncados a 500 chars
- Sin extracciÃ³n de PICO
- Sin extracciÃ³n de resultados numÃ©ricos
- Sin identificaciÃ³n de endpoints
- Sin vinculaciÃ³n snippetâ†’claim

#### Arquitectura CDR Requerida (DSPy)

```python
import dspy

class StudyCardExtractor(dspy.Signature):
    """Extract structured study information from abstract/full-text."""

    text: str = dspy.InputField()
    study_type: str = dspy.OutputField(desc="RCT, Cohort, CaseControl, etc.")
    population: str = dspy.OutputField(desc="Study population characteristics")
    intervention: str = dspy.OutputField(desc="Main intervention/exposure")
    comparator: str = dspy.OutputField(desc="Control/comparison group")
    outcomes: list[str] = dspy.OutputField(desc="Measured outcomes")
    sample_size: int = dspy.OutputField(desc="Number of participants")
    main_finding: str = dspy.OutputField(desc="Primary result with numbers")

class ExtractorModule(dspy.Module):
    def __init__(self):
        self.extractor = dspy.ChainOfThought(StudyCardExtractor)

    def forward(self, record: Record) -> StudyCard:
        result = self.extractor(text=record.abstract)
        return StudyCard(
            record_id=record.record_id,
            study_type=result.study_type,
            population=result.population,
            intervention=result.intervention,
            # ... etc
        )
```

**Gap:** ğŸ”´ CRÃTICO â€” No existe extracciÃ³n estructurada.

---

### 2.5 EvaluaciÃ³n de Sesgo (RoB2)

#### Estado Actual

```python
# ClasificaciÃ³n bÃ¡sica por tipo de publicaciÃ³n (lÃ­neas 278-330)
def _classify_evidence(self, pub_types: list[str], title: str) -> EvidenceLevel:
    """Classify article evidence level based on publication type."""
    if any(t in " ".join(pub_types_lower) for t in ["meta-analysis", "systematic review"]):
        return EvidenceLevel.LEVEL_1A
    if "randomized controlled trial" in " ".join(pub_types_lower):
        return EvidenceLevel.LEVEL_1B
    # ... heurÃ­sticas simples basadas en keywords
```

**Problemas:**

- Solo usa tipo de publicaciÃ³n y tÃ­tulo
- No evalÃºa riesgo de sesgo real
- No sigue metodologÃ­a Cochrane RoB2
- No considera dominios de sesgo especÃ­ficos

#### Arquitectura CDR Requerida

```python
class RoB2Domain(str, Enum):
    RANDOMIZATION = "D1: Randomization process"
    DEVIATIONS = "D2: Deviations from intended interventions"
    MISSING_DATA = "D3: Missing outcome data"
    MEASUREMENT = "D4: Measurement of the outcome"
    SELECTION = "D5: Selection of the reported result"

class RoB2Assessment(BaseModel):
    domain: RoB2Domain
    judgment: Literal["low", "some_concerns", "high"]
    support_text: str
    supporting_quotes: list[Snippet]

class RoB2Result(BaseModel):
    record_id: str
    domain_assessments: list[RoB2Assessment]
    overall_risk: Literal["low", "some_concerns", "high"]
    justification: str

class RoB2Assessor(dspy.Module):
    """Assess risk of bias following Cochrane RoB2 methodology."""

    def assess_domain(self, study_card: StudyCard, domain: RoB2Domain) -> RoB2Assessment:
        # Structured prompting for each domain
        pass

    def assess_overall(self, study_card: StudyCard) -> RoB2Result:
        assessments = [self.assess_domain(study_card, d) for d in RoB2Domain]
        # Overall = highest risk among domains
        pass
```

**Gap:** ğŸ”´ CRÃTICO â€” No existe evaluaciÃ³n de sesgo formal.

---

### 2.6 Capa de VerificaciÃ³n

#### Estado Actual

**NO EXISTE.** El LLM genera texto libre sin verificaciÃ³n.

```python
# El resultado final es simplemente lo que devuelve el LLM
response = await client.post(
    f"{self.api_url}/api/v1/query",
    json={"query": research_prompt, ...}
)
self.research_result = data.get("response", "")  # Texto sin verificar
```

#### Arquitectura CDR Requerida

```python
class Verifier:
    """Verify that each claim has supporting evidence."""

    def verify_claim(self, claim: EvidenceClaim, study_cards: list[StudyCard]) -> VerificationResult:
        # 1. Find supporting snippets
        supporting = self.find_supporting_evidence(claim, study_cards)

        # 2. Check consistency
        consistent = self.check_consistency(claim.claim_text, supporting)

        # 3. Check numerical accuracy
        numbers_ok = self.verify_numbers(claim, supporting)

        return VerificationResult(
            claim=claim,
            verified=consistent and numbers_ok,
            supporting_evidence=supporting,
            issues=self.collect_issues()
        )

    def verify_all(self, claims: list[EvidenceClaim], study_cards: list[StudyCard]) -> list[VerificationResult]:
        results = [self.verify_claim(c, study_cards) for c in claims]
        # Gate: Si >20% claims no verificados, FAIL
        verified_ratio = sum(1 for r in results if r.verified) / len(results)
        if verified_ratio < 0.8:
            raise VerificationGateFailure(f"Only {verified_ratio:.0%} claims verified")
        return results
```

**Gap:** ğŸ”´ CRÃTICO â€” No existe verificaciÃ³n de afirmaciones.

---

### 2.7 Capa de SÃ­ntesis

#### Estado Actual

```python
# El prompt pide al LLM generar todo el contenido (lÃ­neas 750-1000)
prompt = f"""
...
TEMA DE INVESTIGACIÃ“N:
"{context.query}"

LITERATURA RECOPILADA:
{context.synthesized_content}

ESTRUCTURA DEL INFORME:
{structure}  # Secciones sugeridas

GENERA EL INFORME
"""
# El LLM genera TODAS las afirmaciones sin constraint de trazabilidad
```

**Problemas:**

- El LLM puede inventar informaciÃ³n
- No hay vinculaciÃ³n obligatoria claimâ†’source
- No hay cuantificaciÃ³n de incertidumbre
- No hay sÃ­ntesis jerÃ¡rquica (por outcome)

#### Arquitectura CDR Requerida

```python
class ClaimGenerator(dspy.Module):
    """Generate evidence-backed claims with mandatory citations."""

    def generate_claim(
        self,
        question: str,
        study_cards: list[StudyCard],
        outcome: str
    ) -> EvidenceClaim:
        # Solo puede generar claims basados en snippets existentes
        relevant_cards = [c for c in study_cards if outcome in c.outcomes]

        if not relevant_cards:
            return EvidenceClaim(
                claim_text=f"No se encontrÃ³ evidencia sobre {outcome}",
                source_refs=[],
                supporting_snippets=[],
                confidence=0.0,
                uncertainty_note="Sin estudios identificados"
            )

        # Synthesize with mandatory attribution
        claim = self.synthesize_with_citations(relevant_cards)
        claim.supporting_snippets = self.extract_supporting_snippets(relevant_cards)

        return claim

class Synthesizer:
    """Orchestrate claim generation and assembly."""

    def synthesize(self, state: CDRState) -> list[EvidenceClaim]:
        # 1. Identify outcomes to address
        outcomes = self.identify_outcomes(state.pico, state.study_cards)

        # 2. Generate claim per outcome
        claims = []
        for outcome in outcomes:
            claim = self.claim_generator.generate_claim(
                state.question, state.study_cards, outcome
            )
            claims.append(claim)

        # 3. Add uncertainty and limitations
        claims = self.add_uncertainty_notes(claims)

        return claims
```

**Gap:** ğŸ”´ CRÃTICO â€” SÃ­ntesis libre sin trazabilidad.

---

### 2.8 Output/Publisher

#### Estado Actual

```python
# PDF export bÃ¡sico (pdf_export.py)
def generate_research_pdf(query, result, sources, steps, user_mode):
    # Genera PDF con el texto libre del LLM
    # No hay estructura de claims verificados
    # No hay traces de decisiones
```

**Problemas:**

- Output es texto libre
- No hay PRISMA flow diagram
- No hay tabla de evidencia estructurada
- No hay secciÃ³n de traces/auditorÃ­a

#### Arquitectura CDR Requerida

```python
class ReportPublisher:
    """Generate final report with full traceability."""

    def generate_report(self, state: CDRState) -> Report:
        return Report(
            title=f"Systematic Review: {state.question}",
            sections=[
                self.generate_abstract(state),
                self.generate_methods(state),
                self.generate_prisma_diagram(state),
                self.generate_evidence_table(state),
                self.generate_findings(state.claims),
                self.generate_discussion(state),
                self.generate_conclusions(state),
                self.generate_references(state),
                self.generate_appendix_traces(state),
            ],
            metadata=ReportMetadata(
                generated_at=datetime.now(),
                search_date=state.search_plan.executed_at,
                databases_searched=state.search_plan.sources,
                total_records=len(state.retrieved_records),
                included_studies=len(state.study_cards),
            )
        )

    def generate_evidence_table(self, state: CDRState) -> Section:
        """Table with Study, PICO, Findings, RoB2 per study."""
        rows = []
        for card in state.study_cards:
            rows.append({
                "study": f"{card.authors[0]} {card.year}",
                "design": card.study_type,
                "n": card.sample_size,
                "intervention": card.intervention,
                "comparator": card.comparator,
                "outcome": card.primary_endpoint,
                "result": card.effect_size,
                "rob2": card.rob2_result.overall_risk if card.rob2_result else "N/A"
            })
        return Section(type="evidence_table", data=rows)
```

**Gap:** ğŸ”´ CRÃTICO â€” Output no estructurado sin tabla de evidencia ni traces.

---

## 3. Componentes Reutilizables vs Reescribir

### 3.1 Reutilizable (con modificaciones)

| Componente          | Archivo                | AdaptaciÃ³n Necesaria                  |
| ------------------- | ---------------------- | ------------------------------------- |
| PubMed API client   | `scientific_search.py` | AÃ±adir MeSH expansion, batch fetching |
| Evidence level enum | `scientific_search.py` | Expandir con mÃ¡s tipos                |
| DuckDuckGo fallback | `web_search.py`        | Mantener como fallback tier-3         |
| PDF export base     | `pdf_export.py`        | Adaptar para estructura de Report     |
| UI research panel   | `app.py`               | Refactorizar para nuevos estados      |

### 3.2 Descartar y Reescribir

| Componente                           | RazÃ³n                                          |
| ------------------------------------ | ---------------------------------------------- |
| `ScientificArticle` dataclass        | Reemplazar con `Record` + `StudyCard` Pydantic |
| `ScientificResearchContext`          | Reemplazar con `CDRState`                      |
| `perform_scientific_research()`      | Reemplazar con LangGraph orchestration         |
| `build_scientific_research_prompt()` | No aplica en nueva arquitectura                |
| `_synthesize_articles()`             | Reemplazar con extracciÃ³n estructurada         |
| `start_research()` en state.py       | Adaptar para invocar nuevo engine              |

---

## 4. TecnologÃ­as Faltantes (InstalaciÃ³n Requerida)

```bash
# Core orchestration
pip install langgraph langchain-core

# Hybrid retrieval
pip install rank-bm25 faiss-cpu sentence-transformers

# Structured extraction
pip install dspy-ai

# Evaluation
pip install ragas

# Clinical sources
pip install biopython  # For enhanced NCBI access

# Tracing/observability
pip install opentelemetry-api opentelemetry-sdk
```

---

## 5. Plan de ImplementaciÃ³n Propuesto

### Fase 1: Foundation (Semana 1-2)

- [ ] Definir Pydantic schemas (`Record`, `StudyCard`, `EvidenceClaim`, `CDRState`)
- [ ] Configurar LangGraph base con estados mÃ­nimos
- [ ] Implementar `PICOExtractor` con DSPy
- [ ] Tests unitarios de schemas

### Fase 2: Retrieval (Semana 2-3)

- [ ] Implementar BM25 index
- [ ] Integrar PubMedBERT embeddings
- [ ] Implementar reranker (cross-encoder)
- [ ] AÃ±adir ClinicalTrials.gov connector
- [ ] Tests de retrieval quality

### Fase 3: Extraction (Semana 3-4)

- [ ] Implementar `StudyCardExtractor` con DSPy
- [ ] Implementar `SnippetExtractor`
- [ ] Implementar `RoB2Assessor`
- [ ] Tests de extracciÃ³n

### Fase 4: Synthesis & Verification (Semana 4-5)

- [ ] Implementar `ClaimGenerator`
- [ ] Implementar `Verifier` con gates
- [ ] Implementar `Skeptic` layer
- [ ] Tests de verificaciÃ³n

### Fase 5: Integration (Semana 5-6)

- [ ] Integrar todas las capas en LangGraph
- [ ] Implementar `ReportPublisher`
- [ ] Conectar con UI existente
- [ ] End-to-end tests

### Fase 6: Observability & Polish (Semana 6-7)

- [ ] AÃ±adir OpenTelemetry tracing
- [ ] Implementar RAGAs evaluation
- [ ] Optimizar prompts
- [ ] Documentation

---

## 6. Riesgos y Mitigaciones

| Riesgo                   | Probabilidad | Impacto | MitigaciÃ³n                               |
| ------------------------ | ------------ | ------- | ---------------------------------------- |
| LangGraph learning curve | Media        | Alto    | Empezar con grafo simple, iterar         |
| DSPy instability         | Media        | Medio   | Tener fallback a prompts manuales        |
| PubMed rate limits       | Baja         | Medio   | Implementar caching agresivo             |
| Latencia end-to-end      | Alta         | Alto    | Paralelizar donde sea posible, streaming |
| Costo de embeddings      | Media        | Bajo    | Usar modelos open-source                 |

---

## 7. DecisiÃ³n

**Propuesta:** ReimplementaciÃ³n completa del mÃ³dulo Deep Research siguiendo la arquitectura CDR, preservando solo los conectores de API bÃ¡sicos y la estructura de UI.

**JustificaciÃ³n tÃ©cnica:**

1. La arquitectura actual es fundamentalmente incompatible con los requisitos de trazabilidad
2. El costo de adaptar supera el de reescribir
3. Las nuevas dependencias (LangGraph, DSPy) requieren patrones diferentes
4. La calidad SOTA requiere foundation sÃ³lido, no parches

**PrÃ³ximos pasos:**

1. Aprobar este anÃ¡lisis
2. Definir prioridad de fases
3. Comenzar con Fase 1 (schemas + LangGraph base)

---

## ApÃ©ndice A: Archivos Actuales Analizados

```
ui/medex_ui/
â”œâ”€â”€ scientific_search.py  (1092 lÃ­neas) - PubMed + Semantic Scholar clients
â”œâ”€â”€ web_search.py         (343 lÃ­neas)  - DuckDuckGo fallback
â”œâ”€â”€ state.py              (2935 lÃ­neas) - start_research(), clarification flow
â”œâ”€â”€ app.py                (3844 lÃ­neas) - research_panel() UI
â””â”€â”€ pdf_export.py         (~700 lÃ­neas) - PDF generation
```

## ApÃ©ndice B: Arquitectura CDR Target (resumen visual)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CDR ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  User Question â”€â”€â–º PICO Gate â”€â”€â–º Search Plan â”€â”€â–º Hybrid Retrieval  â”‚
â”‚                         â”‚                              â”‚            â”‚
â”‚                         â–¼                              â–¼            â”‚
â”‚                    [Fails if              BM25 + Dense + Rerank    â”‚
â”‚                     not PICO]                     â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              Screening             â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                           StudyCard Extract        â”‚
â”‚                                           (PICO, Results, N)       â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              RoB2 Assess           â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                               Skeptic              â”‚
â”‚                                          (Challenge findings)      â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              Synthesis             â”‚
â”‚                                         (Claims + Snippets)        â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                            Verification            â”‚
â”‚                                         (Fact check claims)        â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              Publisher              â”‚
â”‚                                    (Report + Evidence Table +      â”‚
â”‚                                     PRISMA + Traces)               â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

_Documento generado durante auditorÃ­a de cÃ³digo. Sujeto a revisiÃ³n y aprobaciÃ³n._
